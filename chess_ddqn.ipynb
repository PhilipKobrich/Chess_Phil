{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           1 Import packages                                  #\n",
    "################################################################################\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from chess_gym import chess_gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use a double ended queue (deque) for memory\n",
    "# When memory is full, this will replace the oldest value with the new one\n",
    "from collections import deque\n",
    "\n",
    "# Supress all warnings (e.g. deprecation warnings) for regular use\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                           2 Define model parameters                          #\n",
    "################################################################################\n",
    "\n",
    "# Set whether to display on screen (slows model)\n",
    "DISPLAY_ON_SCREEN = False\n",
    "# Discount rate of future rewards\n",
    "GAMMA = 0.95\n",
    "# Learing rate for neural network\n",
    "LEARNING_RATE = 0.0003\n",
    "# Maximum number of game steps (observation, action, reward, next observation) to keep\n",
    "MEMORY_SIZE = 1000000\n",
    "# Sample batch size for policy network update\n",
    "BATCH_SIZE = 3\n",
    "# Number of game steps to play before starting training (all random actions)\n",
    "REPLAY_START_SIZE = 10\n",
    "# Time step between actions\n",
    "TIME_STEP = 1\n",
    "# Number of steps between policy -> target network update\n",
    "SYNC_TARGET_STEPS = 10\n",
    "# Exploration rate (episolon) is probability of choosign a random action\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.001\n",
    "# Reduction in epsilon with each game step\n",
    "EXPLORATION_DECAY = 0.9\n",
    "# Simulation duration\n",
    "SIM_DURATION = 200\n",
    "# Training episodes\n",
    "TRAINING_EPISODES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                      3 Define DQN (Deep Q Network) class                     #\n",
    "#                    (Used for both policy and target nets)                    #\n",
    "################################################################################\n",
    "\n",
    "from chess_class import chess_class\n",
    "dummy_chess=chess_class()\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    \"\"\"Deep Q Network. Udes for both policy (action) and target (Q) networks.\"\"\"\n",
    "\n",
    "    def __init__(self, observation_space, action_space, neurons_per_layer=1024):\n",
    "        \"\"\"Constructor method. Set up neural nets.\"\"\"\n",
    "\n",
    "        # Set starting exploration rate\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        \n",
    "        # Set up action space (choice of possible actions)\n",
    "        self.action_space = action_space\n",
    "              \n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=observation_space[0], out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "            nn.Linear(15488,action_space)\n",
    "\n",
    "        )\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(observation_space, 1024),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(1024, 2048),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(2048, 4096),\n",
    "        #     nn.Sigmoid(),\n",
    "        #     nn.Linear(4096, action_space)\n",
    "        #     )\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"Act either randomly or by redicting action that gives max Q\"\"\"\n",
    "        \n",
    "        # Act randomly if random number < exploration rate\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action=dummy_chess.choose_random_move_from_obs(observation[0],\"black\")\n",
    "            # action = random.randrange(self.action_space)\n",
    "            \n",
    "        else:\n",
    "            # Otherwise get predicted Q values of actions\n",
    "            q_values = self.net(torch.FloatTensor(observation))\n",
    "            # Get index of action with best Q\n",
    "            action=dummy_chess._action_from_Q_values(observation[0],\"black\",q_values.detach().numpy()[0])\n",
    "            # action = np.argmax(q_values.detach().numpy()[0])\n",
    "        \n",
    "        return  action\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through network\"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                    4 Define policy net training function                     #\n",
    "################################################################################\n",
    "\n",
    "def optimize(policy_net, target_net, memory):\n",
    "    \"\"\"\n",
    "    Update  model by sampling from memory.\n",
    "    Uses policy network to predict best action (best Q).\n",
    "    Uses target network to provide target of Q for the selected next action.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Do not try to train model if memory is less than reqired batch size\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return    \n",
    " \n",
    "    # Reduce exploration rate (exploration rate is stored in policy net)\n",
    "    policy_net.exploration_rate *= EXPLORATION_DECAY\n",
    "    policy_net.exploration_rate = max(EXPLORATION_MIN, \n",
    "                                      policy_net.exploration_rate)\n",
    "    # Sample a random batch from memory\n",
    "    batch = random.sample(memory, BATCH_SIZE)\n",
    "    for observation, action, reward, observation_next, terminal in batch:\n",
    "        \n",
    "        observation_action_values = policy_net(torch.FloatTensor(observation))\n",
    "        \n",
    "        # Get target Q for policy net update\n",
    "       \n",
    "        if not terminal:\n",
    "            # For non-terminal actions get Q from policy net\n",
    "            expected_observation_action_values = policy_net(torch.FloatTensor(observation))\n",
    "            # Detach next observation values from gradients to prevent updates\n",
    "            expected_observation_action_values = expected_observation_action_values.detach()\n",
    "            # Get next observation action with best Q from the policy net (double DQN)\n",
    "            policy_next_observation_values = policy_net(torch.FloatTensor(observation_next))\n",
    "            policy_next_observation_values = policy_next_observation_values.detach()\n",
    "            best_action = np.argmax(policy_next_observation_values[0].numpy())\n",
    "            # Get target net next observation\n",
    "            next_observation_action_values = target_net(torch.FloatTensor(observation_next))\n",
    "            # Use detach again to prevent target net gradients being updated\n",
    "            next_observation_action_values = next_observation_action_values.detach()\n",
    "            best_next_q = next_observation_action_values[0][best_action].numpy()\n",
    "            updated_q = reward + (GAMMA * best_next_q)      \n",
    "            expected_observation_action_values[0][action] = updated_q\n",
    "        else:\n",
    "            # For termal actions Q = reward (-1)\n",
    "            expected_observation_action_values = policy_net(torch.FloatTensor(observation))\n",
    "            # Detach values from gradients to prevent gradient update\n",
    "            expected_observation_action_values = expected_observation_action_values.detach()\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_observation_action_values[0] = reward\n",
    " \n",
    "        # Set net to training mode\n",
    "        policy_net.train()\n",
    "        # Reset net gradients\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        # calculate loss\n",
    "        loss_v = nn.MSELoss()(observation_action_values, expected_observation_action_values)\n",
    "        # Backpropogate loss\n",
    "        loss_v.backward()\n",
    "        # Update network gradients\n",
    "        policy_net.optimizer.step()  \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                            5 Define memory class                             #\n",
    "################################################################################\n",
    "\n",
    "class Memory():\n",
    "    \"\"\"\n",
    "    Replay memory used to train model.\n",
    "    Limited length memory (using deque, double ended queue from collections).\n",
    "      - When memory full deque replaces oldest data with newest.\n",
    "    Holds, observation, action, reward, next observation, and episode done.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor method to initialise replay memory\"\"\"\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "    def remember(self, observation, action, reward, next_observation, done):\n",
    "        \"\"\"observation/action/reward/next_observation/done\"\"\"\n",
    "        self.memory.append((observation, action, reward, next_observation, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, Exploration:  1.000, Average reward: -0.049400Black made an illegal move\n",
      "Run: 2, Exploration:  1.000, Average reward: -0.049625Black made an illegal move\n",
      "Run: 3, Exploration:  0.656, Average reward: -0.049125Black made an illegal move\n",
      "Run: 4, Exploration:  0.314, Average reward: -0.048425Black made an illegal move\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39768/155034959.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;31m# Update policy net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;31m################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39768/1689927103.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(policy_net, target_net, memory)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_next\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mobservation_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Get target Q for policy net update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_39768/75339643.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;34m\"\"\"Forward pass through network\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\envMowi\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Main program loop\"\"\"\n",
    "from chess_gym import chess_gym\n",
    "############################################################################\n",
    "#                          8 Set up environment                            #\n",
    "############################################################################\n",
    "    \n",
    "# Set up game environemnt\n",
    "sim = chess_gym()\n",
    "\n",
    "# Get number of observations returned for observation\n",
    "observation_space = sim.observation_size\n",
    "\n",
    "# Get number of actions possible\n",
    "action_space = sim.action_size\n",
    "\n",
    "############################################################################\n",
    "#                    9 Set up policy and target nets                       #\n",
    "############################################################################\n",
    "\n",
    "# Set up policy and target neural nets\n",
    "policy_net = DQN(observation_space, action_space)\n",
    "target_net = DQN(observation_space, action_space)\n",
    "\n",
    "# Set loss function and optimizer\n",
    "policy_net.optimizer = optim.Adam(\n",
    "        params=policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Copy weights from policy_net to target\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Set target net to eval rather than training mode\n",
    "# We do not train target net - ot is copied from policy net at intervals\n",
    "target_net.eval()\n",
    "\n",
    "############################################################################\n",
    "#                            10 Set up memory                              #\n",
    "############################################################################\n",
    "    \n",
    "# Set up memomry\n",
    "memory = Memory()\n",
    "\n",
    "############################################################################\n",
    "#                     11 Set up + start training loop                      #\n",
    "############################################################################\n",
    "\n",
    "# Set up run counter and learning loop    \n",
    "run = 0\n",
    "all_steps = 0\n",
    "continue_learning = True\n",
    "\n",
    "# Set up list for results\n",
    "results_run = []\n",
    "results_exploration = []\n",
    "results_score = []\n",
    "\n",
    "# Continue repeating games (episodes) until target complete\n",
    "while continue_learning:\n",
    "    \n",
    "    ########################################################################\n",
    "    #                           12 Play episode                            #\n",
    "    ########################################################################\n",
    "    \n",
    "    # Increment run (episode) counter\n",
    "    run += 1\n",
    "    \n",
    "    ########################################################################\n",
    "    #                             13 Reset game                            #\n",
    "    ########################################################################\n",
    "    \n",
    "    # Reset game environment and get first observation observations\n",
    "    observation = sim.reset()\n",
    "\n",
    "    # Trackers for observation\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset total reward\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Reshape observation into 2D array with observation obsverations as first 'row'\n",
    "    # observation = np.reshape(observation, [1, observation_space])\n",
    "    observation = np.reshape(observation, [1]+list(observation_space))\n",
    "    \n",
    "    # Continue loop until episode complete\n",
    "    while True:\n",
    "        \n",
    "    ########################################################################\n",
    "    #                       14 Game episode loop                           #\n",
    "    ########################################################################\n",
    "        \n",
    "        ####################################################################\n",
    "        #                       15 Get action                              #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Get action to take (se eval mode to avoid dropout layers)\n",
    "        policy_net.eval()\n",
    "        action = policy_net.act(observation)\n",
    "        \n",
    "        ####################################################################\n",
    "        #                 16 Play action (get S', R, T)                    #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Act \n",
    "        observation_next, reward, terminal, info = sim.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # # Update trackers\n",
    "        # tolva_acululacion_camion.append(observation_next[0])\n",
    "        # cinta_pre_evicerado.append(observation_next[1])\n",
    "        # tolva_ev_aut.append(observation_next[2])\n",
    "        # tolva_ev_man.append(observation_next[3])\n",
    "        # salida_evicerado.append(observation_next[4])\n",
    "        rewards.append(reward)\n",
    "                                                        \n",
    "        # Reshape observation into 2D array with observation obsverations as first 'row'\n",
    "        observation_next = np.reshape(observation_next, [1]+list(observation_space))\n",
    "        \n",
    "        # Update display if needed\n",
    "        if DISPLAY_ON_SCREEN:\n",
    "            sim.render()\n",
    "        \n",
    "        ####################################################################\n",
    "        #                  17 Add S/A/R/S/T to memory                      #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Record observation, action, reward, new observation & terminal\n",
    "        \n",
    "        memory.remember(observation, action, reward, observation_next, terminal)\n",
    "        \n",
    "        # Update observation\n",
    "        observation = observation_next\n",
    "        \n",
    "        ####################################################################\n",
    "        #                  18 Check for end of episode                     #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Actions to take if end of game episode\n",
    "        if terminal:\n",
    "            # Get exploration rate\n",
    "            exploration = policy_net.exploration_rate\n",
    "            # Clear print row content\n",
    "            clear_row = '\\r' + ' '*79 + '\\r'\n",
    "            print (clear_row, end ='')\n",
    "            print (f'Run: {run}, ', end='')\n",
    "            print (f'Exploration: {exploration: .3f}, ', end='')\n",
    "            average_reward = total_reward/SIM_DURATION\n",
    "            print (f'Average reward: {average_reward:4.6f}', end='')\n",
    "            print(sim.game_status)\n",
    "            # Add to results lists\n",
    "            # results_run.append(run)\n",
    "            # results_exploration.append(exploration)\n",
    "            # results_score.append(total_reward)\n",
    "            \n",
    "            ################################################################\n",
    "            #             18b Check for end of learning                    #\n",
    "            ################################################################\n",
    "            \n",
    "            if run == TRAINING_EPISODES:\n",
    "                continue_learning = False\n",
    "            \n",
    "            # End episode loop\n",
    "            break\n",
    "        \n",
    "        \n",
    "        ####################################################################\n",
    "        #                        19 Update policy net                      #\n",
    "        ####################################################################\n",
    "        \n",
    "        # Avoid training model if memory is not of sufficient length\n",
    "        if len(memory.memory) > REPLAY_START_SIZE:\n",
    "    \n",
    "            # Update policy net\n",
    "            optimize(policy_net, target_net, memory.memory)\n",
    "\n",
    "            ################################################################\n",
    "            #             20 Update target net periodically                #\n",
    "            ################################################################\n",
    "            \n",
    "            # Use load_state_dict method to copy weights from policy net\n",
    "            if all_steps % SYNC_TARGET_STEPS == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "############################################################################\n",
    "#                      21 Learning complete - plot results                 #\n",
    "############################################################################\n",
    "\n",
    "# Add last run to DataFrame. summarise, and return\n",
    "# run_details = pd.DataFrame()\n",
    "# run_details['tolva_acululacion_camion'] = tolva_acululacion_camion \n",
    "# run_details['cinta_pre_evicerado'] = cinta_pre_evicerado\n",
    "# run_details['tolva_ev_aut'] = tolva_ev_aut\n",
    "# run_details['tolva_ev_man'] = tolva_ev_man\n",
    "# run_details['salida_evicerado'] = salida_evicerado\n",
    "# run_details['reward'] = rewards    \n",
    "    \n",
    "# Target reached. Plot results\n",
    "# plot_results(\n",
    "#     results_run, results_exploration, results_score, run_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation[0][:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1]*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66229ff855dc38a95b4fa5f8df9093ccc8ba09f812935176702c6870d23fe95d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('envMowi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
